{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Machine Learning</font>\n",
    "\n",
    "# <font color='blue'>Capítulo 12 - Processamento de Linguagem Natural</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs: Este é um material de bônus incluído neste curso. PyTorch é estudado em detalhes no curso <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-frameworks\">Deep Learning Frameworks</a> e aplicado em PLN no curso <a href=\"https://www.datascienceacademy.com.br/course?courseid=processamento-de-linguagem-natural-e-reconhecimento-de-voz\">Processamento de Linguagem Natural</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudo de Caso - Buscador de Palavras em Texto Por Similaridade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imagens/buscador.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A definição deste estudo de caso está no manual em pdf no Capítulo 12 do Curso de <a href=\"https://www.datascienceacademy.com.br/course?courseid=machine-learning-engineer\">Machine Learning</a>**. \n",
    "\n",
    "Faça a leitura do manual antes de prosseguir com o Estudo de Caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Error while checking for conflicts. Please file an issue on pip's issue tracker: https://github.com/pypa/pip/issues/new\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daholive/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/home/daholive/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daholive/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/home/daholive/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daholive/anaconda3/lib/python3.7/site-packages/pip/_internal/commands/install.py\", line 535, in _determine_conflicts\n",
      "    return check_install_conflicts(to_install)\n",
      "  File \"/home/daholive/anaconda3/lib/python3.7/site-packages/pip/_internal/operations/check.py\", line 108, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"/home/daholive/anaconda3/lib/python3.7/site-packages/pip/_internal/operations/check.py\", line 50, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"/home/daholive/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/home/daholive/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/home/daholive/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/home/daholive/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/home/daholive/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1420, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"/home/daholive/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1616, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/daholive/anaconda3/lib/python3.7/site-packages/queuelib-1.5.0.dist-info/METADATA'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# !pip install torch==1.5.0\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala o PyTorch\n",
    "!pip install -q torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb7131cabf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from nltk.tokenize import word_tokenize\n",
    "%matplotlib inline\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch      1.6.0\n",
      "numpy      1.19.1\n",
      "matplotlib 3.3.1\n",
      "Data Science Academy\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando e Processando os Dados\n",
    "\n",
    "Para este estudo de caso, usaremos o famoso texto de Isaac Asimov: The Last Question.\n",
    "\n",
    "http://users.ece.cmu.edu/~gamvrosi/thelastq.html\n",
    "\n",
    "Traduzimos o texto e usaremos para treinar o modelo GloVe e depois buscar palavras por similaridade. Recomendados a leitura do arquivo asimov.txt (usado na célula abaixo) antes de executar o restante do Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre o arquivo para leitura e carrega na variável arquivo_texto\n",
    "arquivo_texto = open('dados/asimov.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte as palavars para minúsculo\n",
    "texto = arquivo_texto.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fecha o arquivo\n",
    "arquivo_texto.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenização do texto\n",
    "texto_token = word_tokenize(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variável para o comprimento total dos tokens\n",
    "comp_tokens = len(texto_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Tokens:  5282\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de Tokens: \", comp_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando o Vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do Vocabulário: 1397\n"
     ]
    }
   ],
   "source": [
    "# Criando o vocabulário\n",
    "vocab = set(texto_token)\n",
    "vocab_size = len(vocab)\n",
    "print(\"Tamanho do Vocabulário:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para mapear as palavras aos índices\n",
    "palavra_indice = {palavra: i for i, palavra in enumerate(vocab)}\n",
    "#palavra_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para mapear os índices às palavras\n",
    "indice_palavra = {i: palavra for i, palavra in enumerate(vocab)}\n",
    "#indice_palavra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvo indicação em contrário, usamos um contexto de dez palavras à esquerda e dez palavras à direita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho do contexto\n",
    "CONTEXT_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz de co-ocorrência preenchida com zeros\n",
    "co_occ_mat = np.zeros((vocab_size, vocab_size))\n",
    "co_occ_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora percorremos os dicionários de mapeamento criados anteriormente e preenchemos a matriz de co-ocorrência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop externo por todo comprimento do vocabulário\n",
    "for i in range(comp_tokens):\n",
    "    \n",
    "    # Loop interno pelo tamanho do contexto\n",
    "    for dist in range(1, CONTEXT_SIZE + 1):\n",
    "        \n",
    "        # Obtém o índice do token\n",
    "        ix = palavra_indice[texto_token[i]]\n",
    "        \n",
    "        # Se a palara estiver à esquerda, inserimos à esquerda na matriz de co-ocorrência\n",
    "        if i - dist > 0:\n",
    "            left_ix = palavra_indice[texto_token[i - dist]]\n",
    "            co_occ_mat[ix, left_ix] += 1.0 / dist\n",
    "            \n",
    "        # Se a palara estiver à direita, inserimos à direita na matriz de co-ocorrência\n",
    "        if i + dist < len(texto_token):\n",
    "            right_ix = palavra_indice[texto_token[i + dist]]\n",
    "            co_occ_mat[ix, right_ix] += 1.0 / dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz de co-ocorrência\n",
    "co_occ_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposta da matriz de co-ocorrências\n",
    "# Retorna um array 2-D com uma linha para cada elemento não-zero \n",
    "co_occs = np.transpose(np.nonzero(co_occ_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape da Matriz de Co-Ocorrência: (1397, 1397)\n"
     ]
    }
   ],
   "source": [
    "# Print\n",
    "print(\"Shape da Matriz de Co-Ocorrência:\", co_occ_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Co-Ocorrência Não-Zero:\n",
      " [[   0  384]\n",
      " [   0  393]\n",
      " [   0  587]\n",
      " ...\n",
      " [1396 1271]\n",
      " [1396 1342]\n",
      " [1396 1366]]\n"
     ]
    }
   ],
   "source": [
    "# Print\n",
    "print(\"Matriz de Co-Ocorrência Não-Zero:\\n\", co_occs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho da embedding\n",
    "EMBEDDING_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "X_MAX = 100\n",
    "ALPHA = 0.75\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.05\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe para o modelo\n",
    "class Glove(nn.Module):\n",
    "\n",
    "    # Método construtor\n",
    "    def __init__(self, vocab_size, comat, embedding_size, x_max, alpha):\n",
    "        super(Glove, self).__init__()\n",
    "        \n",
    "        # Matriz de embeddings com as palavras centrais\n",
    "        self.embedding_V = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Matriz de embeddings com as palavras de contexto\n",
    "        self.embedding_U = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # Bias\n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "        # Inicializa os parâmtetros (pesos que a rede aprende durante o treinamento)\n",
    "        for params in self.parameters():\n",
    "            nn.init.uniform_(params, a = -0.5, b = 0.5)\n",
    "            \n",
    "        # Define os hiperparâmetros (que controlam o treinamento)\n",
    "        self.x_max = x_max\n",
    "        self.alpha = alpha\n",
    "        self.comat = comat\n",
    "    \n",
    "    # Função de forward\n",
    "    def forward(self, center_word_lookup, context_word_lookup):\n",
    "        \n",
    "        # Matrizes embedding de pesos para centro e contexto\n",
    "        center_embed = self.embedding_V(center_word_lookup)\n",
    "        target_embed = self.embedding_U(context_word_lookup)\n",
    "\n",
    "        # Matrizes embedding de bias para centro e contexto\n",
    "        center_bias = self.v_bias(center_word_lookup).squeeze(1)\n",
    "        target_bias = self.u_bias(context_word_lookup).squeeze(1)\n",
    "\n",
    "        # Elementos da matriz de co-ocorrência\n",
    "        co_occurrences = torch.tensor([self.comat[center_word_lookup[i].item(), context_word_lookup[i].item()]\n",
    "                                       for i in range(BATCH_SIZE)])\n",
    "        \n",
    "        # Carrega os pesos\n",
    "        weights = torch.tensor([self.weight_fn(var) for var in co_occurrences])\n",
    "\n",
    "        # Funçã de perda\n",
    "        loss = torch.sum(torch.pow((torch.sum(center_embed * target_embed, dim = 1)\n",
    "            + center_bias + target_bias) - torch.log(co_occurrences), 2) * weights)\n",
    "        \n",
    "        return loss\n",
    "       \n",
    "    # Definição do peso\n",
    "    def weight_fn(self, x):\n",
    "        if x < self.x_max:\n",
    "            return (x / self.x_max) ** self.alpha\n",
    "        return 1\n",
    "        \n",
    "    # Soma de V e U como nossos vetores de palavras\n",
    "    def embeddings(self):\n",
    "        return self.embedding_V.weight.data + self.embedding_U.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para gerar um bacth de palavras\n",
    "def gera_batch(model, batch_size = BATCH_SIZE):\n",
    "    \n",
    "    # Extrai uma amostra\n",
    "    sample = np.random.choice(np.arange(len(co_occs)), size = batch_size, replace = False)\n",
    "    \n",
    "    # Listas de vetores\n",
    "    v_vecs_ix, u_vecs_ix = [], []\n",
    "    \n",
    "    # Loop pela amostra para gerar os vetores\n",
    "    for chosen in sample:\n",
    "        ind = tuple(co_occs[chosen])  \n",
    "        \n",
    "        lookup_ix_v = ind[0]\n",
    "        lookup_ix_u = ind[1]\n",
    "        \n",
    "        v_vecs_ix.append(lookup_ix_v)\n",
    "        u_vecs_ix.append(lookup_ix_u) \n",
    "        \n",
    "    return torch.tensor(v_vecs_ix), torch.tensor(u_vecs_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para o treinamento\n",
    "def treina_glove(comat):\n",
    "    \n",
    "    # Lista para os erros\n",
    "    losses = []\n",
    "    \n",
    "    # Cria o modelo Glove\n",
    "    model = Glove(vocab_size, comat, embedding_size = EMBEDDING_SIZE, x_max = X_MAX, alpha = ALPHA)\n",
    "    \n",
    "    # Otimizador\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr = LEARNING_RATE)\n",
    "    \n",
    "    # Loop pelo número de épocas\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        # Erro total\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Número de bacthes\n",
    "        num_batches = int(len(texto_token) / BATCH_SIZE)\n",
    "        \n",
    "        # Loop pelos batches\n",
    "        for batch in tqdm(range(num_batches)):\n",
    "            \n",
    "            # Zera os gradientes do modelo\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Obtém o bacth de dados\n",
    "            data = gera_batch(model, BATCH_SIZE)\n",
    "            \n",
    "            # Calcula o erro\n",
    "            loss = model(*data)\n",
    "            \n",
    "            # Executa o backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Otimiza os pesos (aqui é onde ocorre o aprendizado)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Erro total para a epoch\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Erros do modelo\n",
    "        losses.append(total_loss)\n",
    "        \n",
    "        # Print da epoch e erro médio do modelo\n",
    "        print('Epoch : %d, Erro Médio : %.02f' % (epoch, np.mean(losses)))\n",
    "        \n",
    "    return model, losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 171.55it/s]\n",
      "  9%|▉         | 15/165 [00:00<00:01, 144.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Erro Médio : 207.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 187.69it/s]\n",
      " 12%|█▏        | 20/165 [00:00<00:00, 192.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Erro Médio : 189.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 192.15it/s]\n",
      " 12%|█▏        | 19/165 [00:00<00:00, 189.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2, Erro Médio : 181.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 192.20it/s]\n",
      " 10%|█         | 17/165 [00:00<00:00, 166.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3, Erro Médio : 177.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 184.49it/s]\n",
      "  9%|▉         | 15/165 [00:00<00:01, 143.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4, Erro Médio : 168.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 186.09it/s]\n",
      " 11%|█         | 18/165 [00:00<00:00, 177.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5, Erro Médio : 164.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 203.86it/s]\n",
      " 11%|█         | 18/165 [00:00<00:00, 176.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6, Erro Médio : 159.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 203.54it/s]\n",
      " 10%|█         | 17/165 [00:00<00:00, 167.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7, Erro Médio : 154.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 190.27it/s]\n",
      " 12%|█▏        | 20/165 [00:00<00:00, 193.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8, Erro Médio : 151.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 205.33it/s]\n",
      " 13%|█▎        | 21/165 [00:00<00:00, 203.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9, Erro Médio : 147.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 181.02it/s]\n",
      " 12%|█▏        | 20/165 [00:00<00:00, 196.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10, Erro Médio : 144.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 191.24it/s]\n",
      "  9%|▉         | 15/165 [00:00<00:01, 144.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 11, Erro Médio : 141.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 191.47it/s]\n",
      " 12%|█▏        | 20/165 [00:00<00:00, 194.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 12, Erro Médio : 138.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 190.38it/s]\n",
      " 13%|█▎        | 21/165 [00:00<00:00, 200.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 13, Erro Médio : 134.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 203.41it/s]\n",
      " 11%|█         | 18/165 [00:00<00:00, 173.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 14, Erro Médio : 130.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:01<00:00, 161.50it/s]\n",
      "  8%|▊         | 14/165 [00:00<00:01, 136.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 15, Erro Médio : 128.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 197.49it/s]\n",
      " 12%|█▏        | 19/165 [00:00<00:00, 185.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 16, Erro Médio : 125.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 183.79it/s]\n",
      " 12%|█▏        | 20/165 [00:00<00:00, 194.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 17, Erro Médio : 122.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 200.63it/s]\n",
      " 10%|█         | 17/165 [00:00<00:00, 167.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 18, Erro Médio : 119.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 190.32it/s]\n",
      " 12%|█▏        | 20/165 [00:00<00:00, 192.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 19, Erro Médio : 117.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 202.10it/s]\n",
      " 11%|█         | 18/165 [00:00<00:00, 178.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 20, Erro Médio : 115.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 202.17it/s]\n",
      " 12%|█▏        | 19/165 [00:00<00:00, 186.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 21, Erro Médio : 113.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 192.31it/s]\n",
      " 10%|▉         | 16/165 [00:00<00:00, 149.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 22, Erro Médio : 110.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 183.66it/s]\n",
      " 11%|█         | 18/165 [00:00<00:00, 176.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 23, Erro Médio : 109.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 199.54it/s]\n",
      " 11%|█         | 18/165 [00:00<00:00, 173.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 24, Erro Médio : 107.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 189.95it/s]\n",
      " 12%|█▏        | 20/165 [00:00<00:00, 194.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 25, Erro Médio : 105.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 199.71it/s]\n",
      " 12%|█▏        | 19/165 [00:00<00:00, 184.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 26, Erro Médio : 103.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 174.39it/s]\n",
      " 13%|█▎        | 21/165 [00:00<00:00, 204.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 27, Erro Médio : 102.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 195.27it/s]\n",
      " 10%|▉         | 16/165 [00:00<00:00, 152.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 28, Erro Médio : 100.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 191.86it/s]\n",
      " 10%|▉         | 16/165 [00:00<00:00, 159.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 29, Erro Médio : 98.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 197.57it/s]\n",
      " 10%|█         | 17/165 [00:00<00:00, 169.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 30, Erro Médio : 97.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 189.68it/s]\n",
      "  7%|▋         | 12/165 [00:00<00:01, 117.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 31, Erro Médio : 95.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 187.99it/s]\n",
      "  8%|▊         | 14/165 [00:00<00:01, 132.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 32, Erro Médio : 94.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 168.27it/s]\n",
      " 12%|█▏        | 19/165 [00:00<00:00, 180.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 33, Erro Médio : 93.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 194.41it/s]\n",
      " 12%|█▏        | 19/165 [00:00<00:00, 188.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 34, Erro Médio : 91.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 195.48it/s]\n",
      " 10%|█         | 17/165 [00:00<00:00, 166.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 35, Erro Médio : 90.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 194.29it/s]\n",
      " 11%|█         | 18/165 [00:00<00:00, 175.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 36, Erro Médio : 89.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 189.07it/s]\n",
      " 12%|█▏        | 19/165 [00:00<00:00, 186.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 37, Erro Médio : 88.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 182.69it/s]\n",
      " 10%|█         | 17/165 [00:00<00:00, 168.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 38, Erro Médio : 87.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 185.33it/s]\n",
      " 10%|▉         | 16/165 [00:00<00:00, 155.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 39, Erro Médio : 86.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 189.84it/s]\n",
      " 11%|█         | 18/165 [00:00<00:00, 176.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 40, Erro Médio : 85.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 199.51it/s]\n",
      " 10%|▉         | 16/165 [00:00<00:00, 152.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 41, Erro Médio : 83.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 191.42it/s]\n",
      " 10%|▉         | 16/165 [00:00<00:00, 159.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 42, Erro Médio : 82.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 182.98it/s]\n",
      " 11%|█         | 18/165 [00:00<00:00, 173.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 43, Erro Médio : 81.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 179.26it/s]\n",
      " 11%|█         | 18/165 [00:00<00:00, 172.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 44, Erro Médio : 81.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 144/165 [00:00<00:00, 159.80it/s]"
     ]
    }
   ],
   "source": [
    "# Executa a função de treinamento e retorna o modelo e os erros\n",
    "model, losses = treina_glove(co_occ_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para o plot do erro durante o treinamento\n",
    "def plot_loss(losses, title):\n",
    "    plt.plot(range(len(losses)), losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Erro')\n",
    "    plt.title(title)\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plot_loss(losses, \"Erro de Treinamento do Modelo GloVe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando o Modelo: Similaridade de Palavras, analogias de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que retorna a embedding de uma palavra\n",
    "def get_palavra(palavra, modelo, word_to_ix):\n",
    "    return model.embeddings()[word_to_ix[palavra]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para busca a palavra mais próxima\n",
    "def busca_palavra_similaridade(vec, word_to_ix, n = 10):\n",
    "    all_dists = [(w, torch.dist(vec, get_palavra(w, model, palavra_indice))) for w in palavra_indice]\n",
    "    return sorted(all_dists, key = lambda t: t[1])[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando o vetor (embedding) de uma palavra \n",
    "vector = get_palavra(\"espaço\", model, palavra_indice)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca as palavras similares à palavra \"espaço\"\n",
    "busca_palavra_similaridade(vector, palavra_indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que a palavra \"espaço\" tem 0 de distância para si mesma. A próxima palavra mais parecida com \"espaço\" é \"universo\" e assim por diante. Quanto menor a distância, mais parecida a palavra. Lembrando que a busca por similaridade é feita com as embeddings treinadas com o modelo GloVe. \n",
    "\n",
    "Mais um exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando o vetor (embedding) de uma palavra \n",
    "vector = get_palavra(\"solar\", model, palavra_indice)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca as palavras similares à palavra \"solar\"\n",
    "busca_palavra_similaridade(vector, palavra_indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distância da palavra \"solar\" para si mesma é 0 e a palavra com maior similaridade é \"energia\" o que faz todo sentido se você leu o texto do Asimov usado para treinar o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogia\n",
    "\n",
    "![title](imagens/glove.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe na imagem acima que criamos uma \"fórmula\" com 3 palavras visando buscar a quarta palavra, o que é feito por analogia das embeddings (vetores de palavras).\n",
    "\n",
    "Criamos então uma função para buscar a palavra por analogia no formato: \n",
    "\n",
    "palavra1 : palavra2 :: palavra3 : ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para busca de palavra por analogia\n",
    "def busca_analogia(p1, p2, p3, n = 5, filtro = True):\n",
    "    \n",
    "    # Print\n",
    "    print('\\n[%s : %s :: %s : ?]' % (p1, p2, p3))\n",
    "   \n",
    "    # p2 - p1 + p3 = p4\n",
    "    closest_words = busca_palavra_similaridade(get_palavra(p2, model, palavra_indice) - \n",
    "                                               get_palavra(p1, model, palavra_indice) + \n",
    "                                               get_palavra(p3, model, palavra_indice), \n",
    "                                               palavra_indice)\n",
    "    \n",
    "    # Vamos excluir as 3 palavras passadas como parâmetro\n",
    "    if filtro:\n",
    "        closest_words = [t for t in closest_words if t[0] not in [p1, p2, p3]]\n",
    "        \n",
    "    for tuple in closest_words[:n]:\n",
    "        print('(%.4f) %s' % (tuple[1], tuple[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca por analogia\n",
    "busca_analogia(\"família\", \"crianças\", \"humano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E aí estão as palavras que melhor se encaixam na quarta palavra, de acordo com nosso modelo.\n",
    "\n",
    "Quanto maior a distância, menor a similaridade! Treine o modelo com seus próprios textos e experimente a busca por similaridade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
